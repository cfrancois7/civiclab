{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and summarize information with LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you'll find how to run LLM model to extract useful data from human and public contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure LangChain and transformers from huggingface are installed. It is recommanded to install huggingface transformers package from source repo.  \n",
    "\n",
    "`!pip install langchain`  \n",
    "`!pip install git+https://github.com/huggingface/transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine_learning/.anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "from langchain_community.llms import VLLM, VLLMOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"###\n",
    "Context: Le champs d'application est l'analyse de sondage et de consultation publique.\n",
    "Ton: Formel.\n",
    "Audience: Chercheurs ou analystes de sondages.\n",
    "###\n",
    "Objectif:\n",
    "1. Réalise le résumé de <<< TEXT >>> sous forme de synthèse des faits et propositions principaux.\n",
    "2. Liste étape par étape les items principaux. Un item est un fait ou une proposition. Un item n'est pas un exemple.\n",
    "3. Ecrit les items obligatoirement soit sous forme d'une phrase complète avec un sujet, un verbe et un complément, soit commence par un verbe à l'infinitif d'ordre.\n",
    "    - Ne fait pas de sous item.\n",
    "    - Fusionne en un seul item les items très proches.\n",
    "    - Précise si un item est une proposition (\"label\":\"proposition\") ou un fait (\"label\":\"fact\").\n",
    "    - Précise si l'item est syntaxiquement négatif ou positif. Un item négatif contient une négation, par exemple: [\"ne\", \"n'\", \"ne pas\", \"ne plus de\", \"non\"].\n",
    "\n",
    "###\n",
    "Réponse: Strict Format JSON. Exemple: ```\n",
    "{\"summary\": \"Les chats sont très agiles, ils retombent toujours sur leurs pattes. Cependant, ils n'ont pas neuve vies. L'on devrait mieux prendre soin d'eux.\",\n",
    "\"items\":[\n",
    "        {\"name\": \"Les chats retombent sur leurs pattes\", \"negative\":\"false\", \"label\":\"fact\"},\n",
    "        {\"name\": \"Les chats n'ont pas neuve vies\", \"negative\":\"true\", \"label\":\"fact\"},\n",
    "        {\"name\": \"Prendre mieux soin des animaux\", \"negative\":\"false\", \"label\":\"proposition\"}\n",
    "]}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Résume et extrait les items principaux de <<< TEXT >>>.\n",
    "Précise si l'item est syntaxiquement négatif ou non.\n",
    "Répond avec au format JSON.\n",
    "<<< {input} >>>\n",
    "JSON:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 16:19:04,548\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-03 16:19:04 config.py:177] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-03 16:19:04 llm_engine.py:72] Initializing an LLM engine with config: model='TheBloke/NeuralBeagle14-7B-AWQ', tokenizer='TheBloke/NeuralBeagle14-7B-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3000, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, seed=0)\n",
      "INFO 04-03 16:19:08 weight_utils.py:164] Using model weights format ['*.safetensors']\n",
      "INFO 04-03 16:20:08 llm_engine.py:322] # GPU blocks: 704, # CPU blocks: 2048\n",
      "WARNING 04-03 16:20:08 cache_engine.py:100] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 04-03 16:20:08 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-03 16:20:08 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-03 16:20:20 model_runner.py:698] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TheBloke/NeuralBeagle14-7B-AWQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "vllm_kwargs = {\n",
    "    \"quantization\": \"awq\",\n",
    "    \"max_model_len\": 3000,\n",
    "    \"max_context_len_to_capture\":2000,\n",
    "    \"gpu_memory_utilization\":0.9,\n",
    "    # \"enforce_eager\":True,\n",
    "}\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "llm = VLLM(\n",
    "    model=model_name,\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.2,\n",
    "    dtype=\"auto\",\n",
    "    repetition_penalty=1.3,\n",
    "    streamer=streamer,\n",
    "    vllm_kwargs=vllm_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_prompt=1782\n"
     ]
    }
   ],
   "source": [
    "input = \"\"\"\n",
    "réduire drastiquement la fraude fiscale. Imposer les grands groupes (GAFA) qui ne le sont pas suffisamment Renforcer la taxe sur les transactions  financières.\n",
    "\"\"\"\n",
    "\n",
    "question_ = question.format(input=input)\n",
    "template_ = template.format(question=question_, system_message=system_message)\n",
    "\n",
    "length_prompt = len(template_)\n",
    "print(f'{length_prompt=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.84s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"system_message\", \"question\"]\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "result = llm_chain.invoke(\n",
    "    {\n",
    "        \"system_message\": system_message,\n",
    "        \"question\": question.format(input=input),\n",
    "        \"stop\": [\"<|im_end|>\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary\": \"Propositions pour réduire la fraude fiscale et imposer les grands groupes (GAFA): 1. Réduire drastiquement la fraude fiscale, 2. Imposer les grands groupes non payants, 3. Renforcer la taxe sur les transactions financières.\",\n",
      "\"items\": [\n",
      "    {\n",
      "        \"name\": \"Réduire drastiquement la fraude fiscale\",\n",
      "        \"negative\": \"false\",\n",
      "        \"label\": \"fact\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Imposer les grands groupes non payants\",\n",
      "        \"negative\": \"false\",\n",
      "        \"label\": \"fact\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Renforcer la taxe sur les transactions financières\",\n",
      "        \"negative\": \"false\",\n",
      "        \"label\": \"proposition\"\n",
      "    ]\n",
      "]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia4gov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
