{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and summarize information with LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you'll find how to run LLM model to extract useful data from human and public contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure LangChain and transformers from huggingface are installed. It is recommanded to install huggingface transformers package from source repo.  \n",
    "\n",
    "`!pip install langchain`  \n",
    "`!pip install git+https://github.com/huggingface/transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "from langchain_community.llms import VLLM, VLLMOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on local webserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on the local server with this command. For instance:\n",
    "```\n",
    " python -m vllm.entrypoints.openai.api_server\\\n",
    "    --model TheBloke/NeuralBeagle14-7B-AWQ\\\n",
    "    --chat-template ./config/template_chatml.jinja\\\n",
    "    --quantization awq\\\n",
    "    --trust-remote-code\\\n",
    "    --max-model-len 2048\n",
    "```\n",
    "\n",
    "or more simple, execute the bash script `run_server.sh`. To do:\n",
    "1. open a terminal and go to the directory of the project\n",
    "2. execute: `$ sh run_server.sh`\n",
    "\n",
    "*Warning*: the context from this tutorial take at leat 450 tokens. Process only text with less than 1500 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the config corresponding to the LLM model you want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "with open('../config/local_llm.toml', 'rb') as file:\n",
    "    configs = tomllib.load(file)\n",
    "\n",
    "print(configs)  # the config file is a dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the template to structure the query. The model used in this tutorial is based on Mistral 7B, so used ChatML structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\n",
    "Progressivité réelle de l'impôt sur le revenu sans en passer par les tranches mais en s'appuyant, par exemple,  sur un coefficient variable suivant le niveau de revenu (ou bien s'inspirer librement du modèle suédois).\n",
    "-Taxation des revenus financiers issus de placements qui ne sont pas directement investis dans l'économie (exemple: les produits dérivés).\n",
    "-imposer les ayants droits aux minima sociaux à raison d'une somme symbolique: 30 ou 50 euros par an par exemple.\n",
    "-faire sauter certaines niches fiscales après audit de la Cour des comptes.\n",
    "-taxer comme le font les USA les Français qui prennent une autre nationalité ou résident à l'étranger.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "model_name = configs[\"model\"][\"name\"]\n",
    "user_message = configs[\"template\"][\"user\"].format(input=input)\n",
    "system_message = configs[\"template\"][\"system\"]\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=configs[\"server\"][\"base_url\"],\n",
    "    api_key=configs[\"server\"][\"api_key\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=configs[\"model\"][\"name\"],\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    stop=configs[\"model\"][\"stop\"],\n",
    "    top_p=configs[\"model\"][\"top_p\"],\n",
    "    temperature=configs[\"model\"][\"temperature\"],\n",
    ")\n",
    "\n",
    "output = completion.choices[0].message.content.split(\"```\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia4gov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
