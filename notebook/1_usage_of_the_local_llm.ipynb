{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and summarize information with LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you'll find how to run LLM model to extract useful data from human and public contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure LangChain and transformers from huggingface are installed. It is recommanded to install huggingface transformers package from source repo.  \n",
    "\n",
    "`!pip install langchain`  \n",
    "`!pip install git+https://github.com/huggingface/transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine_learning/.anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline\n",
    "from langchain_community.llms import VLLM, VLLMOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on local webserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on the local server with this command. For instance:\n",
    "```\n",
    " python -m vllm.entrypoints.openai.api_server\\\n",
    "    --model TheBloke/NeuralBeagle14-7B-AWQ\\\n",
    "    --chat-template ./config/template_chatml.jinja\\\n",
    "    --quantization awq\\\n",
    "    --trust-remote-code\\\n",
    "    --max-model-len 2048\n",
    "```\n",
    "\n",
    "or more simple, execute the bash script `run_server.sh`. To do:\n",
    "1. open a terminal and go to the directory of the project\n",
    "2. execute: `$ sh run_server.sh`\n",
    "\n",
    "*Warning*: the context from this tutorial take at leat 450 tokens. Process only text with less than 1500 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the config corresponding to the LLM model you want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'server': {'base_url': 'http://localhost:8000/v1', 'api_key': 'EMPTY'}, 'model': {'name': 'TheBloke/NeuralBeagle14-7B-AWQ', 'stop': '<|im_end|>', 'top_p': 0.95, 'temperature': 0}, 'template': {'system': '###\\nContext: Le champs d\\'application est l\\'analyse de sondage et de consultation publique.\\nTon: Formel.\\nAudience: Chercheurs ou analystes de sondages.\\n###\\nObjectif:\\n1. Réalise le résumé de <<< TEXT >>> sous forme de synthèse des faits et propositions principaux.\\n2. Exploite seulement le contenu de <<< TEXT >>>.\\n3. Liste étape par étape les items principaux. Un item est un fait ou une proposition. Un item n\\'est pas un exemple.\\n4. Ecrit les items obligatoirement soit sous forme d\\'une phrase complète avec un sujet, un verbe et un complément, soit commence par un verbe à l\\'infinitif d\\'ordre.\\n    - Ne fait pas de sous item.\\n    - Fusionne en un seul item les items très proches.\\n    - Précise si un item est une proposition (\"label\":\"proposition\") ou un fait (\"label\":\"fact\").\\n    - Précise si l\\'item est syntaxiquement négatif ou positif. Un item négatif contient une négation, par exemple: [\"ne\", \"n\\'\", \"ne pas\", \"ne plus de\", \"non\"].\\n###\\nRéponse: Strict Format JSON. Exemple:\\n```{\"summary\": \"Les chats sont très agiles, ils retombent toujours sur leurs pattes. Cependant, ils n\\'ont pas neuve vies. L\\'on devrait mieux prendre soin d\\'eux.\",\\n\"items\":[\\n        {\"name\": \"Les chats retombent sur leurs pattes\", \"negative\":\"false\", \"label\":\"fact\"},\\n        {\"name\": \"Les chats n\\'ont pas neuve vies\", \"negative\":\"true\", \"label\":\"fact\"},\\n        {\"name\": \"Prendre mieux soin des animaux\", \"negative\":\"false\", \"label\":\"proposition\"}\\n]}```\\n', 'user': \"Résume et extrait les items principaux de <<< TEXT >>>.\\nPrécise si l'item est syntaxiquement négatif ou non.\\nRépond avec au format JSON.\\n<<< {input} >>>\\nJSON:\"}}\n"
     ]
    }
   ],
   "source": [
    "import tomllib\n",
    "\n",
    "with open('../config/local_llm.toml', 'rb') as file:\n",
    "    configs = tomllib.load(file)\n",
    "\n",
    "print(configs)  # the config file is a dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the template to structure the query. The model used in this tutorial is based on Mistral 7B, so used ChatML structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\n",
    "Progressivité réelle de l'impôt sur le revenu sans en passer par les tranches mais en s'appuyant, par exemple,  sur un coefficient variable suivant le niveau de revenu (ou bien s'inspirer librement du modèle suédois).\n",
    "-Taxation des revenus financiers issus de placements qui ne sont pas directement investis dans l'économie (exemple: les produits dérivés).\n",
    "-imposer les ayants droits aux minima sociaux à raison d'une somme symbolique: 30 ou 50 euros par an par exemple.\n",
    "-faire sauter certaines niches fiscales après audit de la Cour des comptes.\n",
    "-taxer comme le font les USA les Français qui prennent une autre nationalité ou résident à l'étranger.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "model_name = configs[\"model\"][\"name\"]\n",
    "user_message = configs[\"template\"][\"user\"].format(input=input)\n",
    "system_message = configs[\"template\"][\"system\"]\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=configs[\"server\"][\"base_url\"],\n",
    "    api_key=configs[\"server\"][\"api_key\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=configs[\"model\"][\"name\"],\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    "    stop=configs[\"model\"][\"stop\"],\n",
    "    top_p=configs[\"model\"][\"top_p\"],\n",
    "    temperature=configs[\"model\"][\"temperature\"],\n",
    ")\n",
    "\n",
    "output = completion.choices[0].message.content.split(\"```\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"summary\": \"Propositions d\\'amélioration fiscale: progressivité sans '\n",
      " 'tranches, taxer revenus financiers non investis, imposer ayants droits aux '\n",
      " 'minima sociaux, sauter niches fiscales après audit, taxer Français à nouveau '\n",
      " 'nationalité ou résidence étrangère.\",\\n'\n",
      " '\"items\": [\\n'\n",
      " '    {\\n'\n",
      " '        \"name\": \"Progressivité sans tranches, coefficient variable suivant '\n",
      " 'le niveau de revenu\",\\n'\n",
      " '        \"negative\": \"false\",\\n'\n",
      " '        \"label\": \"fact\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '        \"name\": \"Taxer revenus financiers non investis dans l\\'économie '\n",
      " '(exemples: produits dérivés)\",\\n'\n",
      " '        \"negative\": \"false\",\\n'\n",
      " '        \"label\": \"fact\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '        \"name\": \"Imposer ayants droits aux minima sociaux (30 ou 50 euros '\n",
      " 'par an)\",\\n'\n",
      " '        \"negative\": \"false\",\\n'\n",
      " '        \"label\": \"proposition\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '        \"name\": \"Sauter niches fiscales après audit de la Cour des '\n",
      " 'comptes\",\\n'\n",
      " '        \"negative\": \"false\",\\n'\n",
      " '        \"label\": \"proposition\"\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '        \"name\": \"Taxer Français prenant autre nationalité ou résidant à '\n",
      " 'l\\'étranger\",\\n'\n",
      " '        \"negative\": \"false\",\\n'\n",
      " '        \"label\": \"proposition\"\\n'\n",
      " '    }\\n'\n",
      " ']}')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia4gov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
